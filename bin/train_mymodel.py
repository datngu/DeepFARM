#!/usr/bin/env python

from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, Embedding
from tensorflow.keras.models import Model
from keras.models import load_model as keras_load_model
from keras.layers import Activation, Flatten, Conv1D, MaxPooling1D, Input, Lambda
from keras import metrics
from keras.callbacks import ModelCheckpoint, EarlyStopping
import tensorflow as tf
import argparse
import os, sys
import numpy as np
import pandas as pd


parser = argparse.ArgumentParser(description = "Train my model...)", formatter_class=argparse.ArgumentDefaultsHelpFormatter)
parser.add_argument("--train", nargs='+', help = "training data *.tfr generated by 'generate_tfr.py' ", required = True)
parser.add_argument("--val", nargs='+', help = "validation data *.tfr generated by 'generate_tfr.py", required = True)
parser.add_argument("--out", default = "DeepFARM_model", help = "model output files - only best model saved")
parser.add_argument('--batch_size', type=int, default = 1024, help = 'batch size for training')
parser.add_argument('--lr', type=float, default = 1e-3, help = 'learning rate')
parser.add_argument('--threads', type=int, default = 32, help = 'CPU cores for data pipeline loading')

args = parser.parse_args()

train_files = args.train
val_files = args.val
out = args.out
batch_size = args.batch_size
num_threads = args.threads



out_model = out + '.h5'
out_hist = out + '.csv'

# Decoding function
def parse_record(record):
    name_to_features = {
        'seq': tf.io.FixedLenFeature([], tf.string),
        'label': tf.io.FixedLenFeature([], tf.string),
    }
    return tf.io.parse_single_example(record, name_to_features)



def decode_record(record):
    seq = tf.io.decode_raw(
        record['seq'], out_type=tf.float16, little_endian=True, fixed_length=None, name=None
    )
    label = tf.io.decode_raw(
        record['label'], out_type=tf.int8, little_endian=True, fixed_length=None, name=None
    )
    seq = tf.reshape(seq, [-1,4])
    #label = tf.cast(label, tf.float16)
    return (seq, label)



def get_dataset(record_file, num_threads = 8, batch = 512):
    dataset = tf.data.TFRecordDataset(record_file, num_parallel_reads = num_threads, compression_type = 'GZIP')
    dataset = dataset.map(parse_record, num_parallel_calls = num_threads)
    dataset = dataset.map(decode_record, num_parallel_calls = num_threads)
    dataset = dataset.shuffle(buffer_size = batch*10).batch(batch)
    return dataset
    


class PositionalEncoding(tf.keras.layers.Layer):
    def __init__(self, max_len, embed_dim):
        super().__init__()
        self.max_len = max_len
        self.embed_dim = embed_dim
        # Generate positional encoding
        position = np.arange(max_len)[:, np.newaxis]  # Shape: (max_len, 1)
        div_term = np.exp(np.arange(0, embed_dim, 2) * -(np.log(10000.0) / embed_dim))
        pe = np.zeros((max_len, embed_dim))
        pe[:, 0::2] = np.sin(position * div_term)
        pe[:, 1::2] = np.cos(position * div_term)
        self.pe = tf.constant(pe, dtype=tf.float32)  # Store as a fixed tensor

    def call(self, x):
        # Add positional encoding (assumes input shape is (batch_size, seq_len, embed_dim))
        seq_len = tf.shape(x)[1]
        return x + self.pe[:seq_len, :]

class TransformerEncoder(tf.keras.layers.Layer):
    def __init__(self, head_size, num_heads, ff_dim, dropout=0.1, **kwargs):
        super().__init__(**kwargs)
        self.attention = MultiHeadAttention(key_dim=head_size, num_heads=num_heads)
        self.dropout1 = Dropout(dropout)
        self.norm1 = LayerNormalization(epsilon=1e-6)

        self.ffn = tf.keras.Sequential([
            Dense(ff_dim, activation="relu"),
            Dense(head_size)
        ])
        self.dropout2 = Dropout(dropout)
        self.norm2 = LayerNormalization(epsilon=1e-6)

    def call(self, inputs, training=False):
        attn_output = self.attention(inputs, inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.norm1(inputs + attn_output)
        
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.norm2(out1 + ffn_output)



def build_model(learning_rate):
    #NUM_INPUT = 1000
    #DENSE_UNITS = 925
    EMBED_DIM = 320
    inp = Input(shape=(NUM_INPUT, 4))
    x = Conv1D(filters=EMBED_DIM, kernel_size=26, activation="relu", padding="valid", name="conv1d_1")(inp)
    x = MaxPooling1D(pool_size=13, strides=13, name="maxpool_1")(x)
    x = Dropout(0.2, name="dropout_1")(x)
    x = PositionalEncoding(1000, EMBED_DIM)(x)
    x = TransformerEncoder(EMBED_DIM, 8, 2048)(x)
    x = Dropout(0.5, name="dropout_2")(x)
    x = Flatten(name="flatten")(x)
    x = Dense(DENSE_UNITS, activation="relu", name="dense_1")(x)
    out = Dense(NUM_OUTPUT, activation="sigmoid", name="dense_2")(x)
    model = Model(inputs=[inp], outputs=[out])
    model.compile(
        optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate, weight_decay= 1e-6),
        loss = tf.keras.losses.BinaryCrossentropy(),
        metrics = ['binary_accuracy', tf.keras.metrics.AUC(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]
    )
    return model


## getting dims of input/output data:

data_dim = get_dataset(val_files, batch = 1)
dim_it = iter(data_dim)
dim_sample = next(dim_it)

DENSE_UNITS = 925
NUM_INPUT = dim_sample[0].shape[1]
NUM_OUTPUT = dim_sample[1].shape[1]


model = build_model(args.lr)

checkpointer = ModelCheckpoint(filepath = out_model, verbose=1, save_best_only=True)
earlystopper = EarlyStopping(monitor="val_loss", patience=5, verbose=1)

train = get_dataset(train_files, batch= batch_size, num_threads = num_threads)
val = get_dataset(val_files, batch= batch_size, num_threads = num_threads)


history = model.fit(train, epochs=100, validation_data=val, callbacks=[checkpointer, earlystopper])


## export history
hist_df = pd.DataFrame(history.history) 
hist_df.to_csv(out_hist)

